{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/kolade/Documents/AML/intro to DL/week 2\")\n",
    "import tqdm_utils\n",
    "#import download_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'download_utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-284-4e946b1d421f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# use the preloaded keras datasets and models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdownload_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlink_all_keras_resources\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'download_utils' is not defined"
     ]
    }
   ],
   "source": [
    "# use the preloaded keras datasets and models\n",
    "download_utils.link_all_keras_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " main class: a layer that can do .forward() and .backward() passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    A building block. Each layer is capable of performing two things:\n",
    "    \n",
    "    - Process input to get output:           output = layer.forward(input)\n",
    "    \n",
    "    - Propagate gradients through itself:    grad_input = layer.backward(input, grad_output)\n",
    "    \n",
    "    Some layers also have learnable parameters which they update during layer.backward.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"Here you can initialize layer parameters (if any) and auxiliary stuff.\"\"\"\n",
    "        # A dummy layer does nothing\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Takes input data of shape [batch, input_units], returns output data [batch, output_units]\n",
    "        \"\"\"\n",
    "        # A dummy layer just returns whatever it gets as input.\n",
    "        return input\n",
    "\n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"\n",
    "        Performs a backpropagation step through the layer, with respect to the given input.\n",
    "        \n",
    "        To compute loss gradients w.r.t input, you need to apply chain rule (backprop):\n",
    "        \n",
    "        d loss / d x  = (d loss / d layer) * (d layer / d x)\n",
    "        \n",
    "        Luckily, you already receive d loss / d layer as input, so you only need to multiply it by d layer / d x.\n",
    "        \n",
    "        If your layer has parameters (e.g. dense layer), you also need to update them here using d loss / d layer\n",
    "        \"\"\"\n",
    "        num_units = input.shape[1]\n",
    "        \n",
    "        d_layer_d_input = np.eye(num_units)\n",
    "        \n",
    "        return np.dot(grad_output, d_layer_d_input) # chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The road ahead\n",
    "\n",
    "building a neural network that classifies MNIST digits. To do so, we'll need a few building blocks:\n",
    "- Dense layer - a fully-connected layer, $f(X)=W \\cdot X + \\vec{b}$\n",
    "- ReLU layer (or any other nonlinearity you want)\n",
    "- Loss function - crossentropy\n",
    "- Backprop algorithm - a stochastic gradient descent with backpropageted gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonlinearity layer\n",
    "\n",
    " it simply applies a nonlinearity to each element of your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Layer):\n",
    "    def __init__(self):\n",
    "        \"\"\"ReLU layer simply applies elementwise rectified linear unit to all inputs\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"Apply elementwise ReLU to [batch, input_units] matrix\"\"\"\n",
    "        return np.maximum(np.zeros_like(input),input)\n",
    "    \n",
    "    def backward(self, input, grad_output):\n",
    "        \"\"\"Compute gradient of loss w.r.t. ReLU input\"\"\"\n",
    "        relu_grad = input > 0\n",
    "        return grad_output*relu_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some tests\n",
    "from util import eval_numerical_gradient\n",
    "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "l = ReLU()\n",
    "grads = l.backward(x,np.ones([10,32])/(32*10))\n",
    "numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).mean(), x=x)\n",
    "assert np.allclose(grads, numeric_grads, rtol=1e-3, atol=0),\\\n",
    "    \"gradient returned by your layer does not match the numerically computed gradient\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense layer\n",
    "\n",
    "Now let's build something more complicated. Unlike nonlinearity, a dense layer actually has something to learn.\n",
    "\n",
    "A dense layer applies affine transformation. In a vectorized form, it can be described as:\n",
    "$$f(X)= W \\cdot X + \\vec b $$\n",
    "\n",
    "Where \n",
    "* X is an object-feature matrix of shape [batch_size, num_features],\n",
    "* W is a weight matrix [num_features, num_outputs] \n",
    "* and b is a vector of num_outputs biases.\n",
    "\n",
    "Both W and b are initialized during layer creation and updated each time backward is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_units, output_units, learning_rate=0.1):\n",
    "        \"\"\"\n",
    "        A dense layer is a layer which performs a learned affine transformation:\n",
    "        f(x) = <W*x> + b\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.input_units = input_units\n",
    "        self.output_units = output_units\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.weights = np.random.rand(input_units, output_units)*0.01\n",
    "        self.biases = np.zeros(output_units)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        \"\"\"\n",
    "        Perform an affine transformation:\n",
    "        f(x) = <W*x> + b\n",
    "        \n",
    "        input shape: [batch, input_units]\n",
    "        output shape: [batch, output units]\n",
    "        \"\"\"\n",
    "        fx=np.dot(input,self.weights)+self.biases\n",
    "        return fx\n",
    "    \n",
    "    def backward(self,input,grad_output):\n",
    "        # compute d f / d x = d f / d dense * d dense / d x\n",
    "        # where d dense/ d x = weights transposed\n",
    "        grad_input = np.dot(grad_output,self.weights.T )\n",
    "        \n",
    "        # compute gradient w.r.t. weights and biases\n",
    "        grad_weights = np.dot(input.T,grad_output)\n",
    "        grad_biases = np.dot(grad_output.T,np.ones(grad_output.shape[0]))\n",
    "\n",
    "        assert grad_weights.shape == self.weights.shape and grad_biases.shape == self.biases.shape\n",
    "        \n",
    "        \n",
    "        self.weights = self.weights - self.learning_rate * grad_weights\n",
    "        self.biases = self.biases - self.learning_rate * grad_biases\n",
    "        \n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the dense layer\n",
    "\n",
    "Here we have a few tests to make sure your dense layer works properly. You can just run them, get 3 \"well done\"s and forget they ever existed.\n",
    "\n",
    "... or not get 3 \"well done\"s and go fix stuff. If that is the case, here are some tips for you:\n",
    "* Make sure you compute gradients for W and b as __sum of gradients over batch__, not mean over gradients. Grad_output is already divided by batch size.\n",
    "* If you're debugging, try saving gradients in class fields, like \"self.grad_w = grad_w\" or print first 3-5 weights. This helps debugging.\n",
    "* If nothing else helps, try ignoring tests and proceed to network training. If it trains alright, you may be off by something that does not affect network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good to go!\n"
     ]
    }
   ],
   "source": [
    "l = Dense(128, 150)\n",
    "\n",
    "assert -0.05 < l.weights.mean() < 0.05 and 1e-3 < l.weights.std() < 1e-1,\\\n",
    "    \"The initial weights must have zero mean and small variance. \"\\\n",
    "    \"If you know what you're doing, remove this assertion.\"\n",
    "assert -0.05 < l.biases.mean() < 0.05, \"Biases must be zero mean. Ignore if you have a reason to do otherwise.\"\n",
    "\n",
    "# To test the outputs, we explicitly set weights with fixed values. DO NOT DO THAT IN ACTUAL NETWORK!\n",
    "l = Dense(3,4)\n",
    "\n",
    "x = np.linspace(-1,1,2*3).reshape([2,3])\n",
    "l.weights = np.linspace(-1,1,3*4).reshape([3,4])\n",
    "l.biases = np.linspace(-1,1,4)\n",
    "\n",
    "assert np.allclose(l.forward(x),np.array([[ 0.07272727,  0.41212121,  0.75151515,  1.09090909],\n",
    "                                          [-0.90909091,  0.08484848,  1.07878788,  2.07272727]]))\n",
    "print(\"good to go!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good to go\n"
     ]
    }
   ],
   "source": [
    "# To test the grads,  gradients is obtained via finite differences\n",
    "\n",
    "from util import eval_numerical_gradient\n",
    "\n",
    "x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "l = Dense(32,64,learning_rate=0)\n",
    "\n",
    "numeric_grads = eval_numerical_gradient(lambda x: l.forward(x).sum(),x)\n",
    "grads = l.backward(x,np.ones([10,64]))\n",
    "\n",
    "assert np.allclose(grads,numeric_grads,rtol=1e-3,atol=0), \"input gradient does not match numeric grad\"\n",
    "print(\"good to go\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good to go!\n"
     ]
    }
   ],
   "source": [
    "#test gradients w.r.t. params\n",
    "def compute_out_given_wb(w,b):\n",
    "    l = Dense(32,64,learning_rate=1)\n",
    "    l.weights = np.array(w)\n",
    "    l.biases = np.array(b)\n",
    "    x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "    return l.forward(x)\n",
    "    \n",
    "def compute_grad_by_params(w,b):\n",
    "    l = Dense(32,64,learning_rate=1)\n",
    "    l.weights = np.array(w)\n",
    "    l.biases = np.array(b)\n",
    "    x = np.linspace(-1,1,10*32).reshape([10,32])\n",
    "    l.backward(x,np.ones([10,64]) / 10.)\n",
    "    return w - l.weights, b - l.biases\n",
    "    \n",
    "w,b = np.random.randn(32,64), np.linspace(-1,1,64)\n",
    "\n",
    "numeric_dw = eval_numerical_gradient(lambda w: compute_out_given_wb(w,b).mean(0).sum(),w )\n",
    "numeric_db = eval_numerical_gradient(lambda b: compute_out_given_wb(w,b).mean(0).sum(),b )\n",
    "grad_w,grad_b = compute_grad_by_params(w,b)\n",
    "\n",
    "assert np.allclose(numeric_dw,grad_w,rtol=1e-3,atol=0), \"weight gradient does not match numeric weight gradient\"\n",
    "assert np.allclose(numeric_db,grad_b,rtol=1e-3,atol=0), \"weight gradient does not match numeric weight gradient\"\n",
    "print(\"good to go!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The loss function\n",
    "\n",
    " the expression for crossentropy as a function of softmax logits (a), :\n",
    "\n",
    "$$ loss = - log \\space {e^{a_{correct}} \\over {\\underset i \\sum e^{a_i} } } $$\n",
    " it can be rewritten as:\n",
    "\n",
    "$$ loss = - a_{correct} + log {\\underset i \\sum e^{a_i} } $$\n",
    "\n",
    "It's called Log-softmax and it's better than naive log(softmax(a)) in all aspects:\n",
    "* Better numerical stability\n",
    "* Easier to get derivative right\n",
    "* Marginally faster to compute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    \"\"\"Compute crossentropy from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    logits_for_answers = logits[np.arange(len(logits)),reference_answers]\n",
    "    \n",
    "    xentropy = - logits_for_answers + np.log(np.sum(np.exp(logits),axis=-1))\n",
    "    \n",
    "    return xentropy\n",
    "\n",
    "def grad_softmax_crossentropy_with_logits(logits,reference_answers):\n",
    "    \"\"\"Compute crossentropy gradient from logits[batch,n_classes] and ids of correct answers\"\"\"\n",
    "    ones_for_answers = np.zeros_like(logits)\n",
    "    ones_for_answers[np.arange(len(logits)),reference_answers] = 1\n",
    "    \n",
    "    softmax = np.exp(logits) / np.exp(logits).sum(axis=-1,keepdims=True)\n",
    "    \n",
    "    return (- ones_for_answers + softmax) / logits.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = np.linspace(-1,1,500).reshape([50,10])\n",
    "answers = np.arange(50)%10\n",
    "\n",
    "softmax_crossentropy_with_logits(logits,answers)\n",
    "grads = grad_softmax_crossentropy_with_logits(logits,answers)\n",
    "numeric_grads = eval_numerical_gradient(lambda l: softmax_crossentropy_with_logits(l,answers).mean(),logits)\n",
    "\n",
    "assert np.allclose(numeric_grads,grads,rtol=1e-3,atol=0), \"The reference implementation has just failed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from preprocessed_mnist import load_dataset\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_dataset(flatten=True)\n",
    "\n",
    "plt.figure(figsize=[6,6])\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.title(\"Label: %i\"%y_train[i])\n",
    "    plt.imshow(X_train[i].reshape([28,28]),cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = []\n",
    "network.append(Dense(X_train.shape[1],100))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(100,200))\n",
    "network.append(ReLU())\n",
    "network.append(Dense(200,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.Dense object at 0x7f6479cff898>\n",
      "<__main__.Dense object at 0x7f6479cff898>\n",
      "<__main__.Dense object at 0x7f6479cff898>\n",
      "<__main__.Dense object at 0x7f6479cff898>\n",
      "<__main__.Dense object at 0x7f6479cff898>\n"
     ]
    }
   ],
   "source": [
    "for i in network:\n",
    "    print ( network[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(network, X):\n",
    "    \"\"\"\n",
    "    Compute activations of all network layers by applying them sequentially.\n",
    "    Return a list of activations for each layer. \n",
    "    Make sure last activation corresponds to network logits.\n",
    "    \"\"\"\n",
    "    input = X\n",
    "    activations = []\n",
    "    layer_output = network[0].forward(input)\n",
    "\n",
    "    activations.append(layer_output)\n",
    "    for i in range(1, len(network)-1):\n",
    "        layer_output = network[i].forward(activations[-1])\n",
    "        activations.append(layer_output)\n",
    "    last_layer_output = network[len(network)-1].forward(activations[-1])\n",
    "    activations.append(np.exp(last_layer_output))\n",
    "    assert len(activations) == len(network)\n",
    "    return activations\n",
    "\n",
    "\n",
    "def predict(network, X):\n",
    "    \"\"\"\n",
    "    Compute network predictions.\n",
    "    \"\"\"\n",
    "    logits = forward(network, X)[-1]\n",
    "    return logits.argmax(axis=-1)\n",
    "\n",
    "\n",
    "def train(network, X, y):\n",
    "    \"\"\"\n",
    "    Train your network on a given batch of X and y.\n",
    "    You first need to run forward to get all layer activations.\n",
    "    Then you can run layer.backward going from last to first layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the layer activations\n",
    "    layer_activations = forward(network, X)\n",
    "\n",
    "    # layer_input[i] is an input for network[i]\n",
    "    layer_inputs = [X]+layer_activations\n",
    "    logits = layer_activations[-1]\n",
    "\n",
    "    # Compute the loss and the initial gradient\n",
    "    loss = softmax_crossentropy_with_logits(logits, y)\n",
    "\n",
    "    loss_grad = grad_softmax_crossentropy_with_logits(logits, y)\n",
    "\n",
    "    dz = loss_grad\n",
    "    for i in reversed(range(len(network))):\n",
    "        dz = network[i].backward(layer_inputs[i], dz)\n",
    "\n",
    "    return np.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    " split data into minibatches, feed each such minibatch into the network and update weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.random.permutation(len(inputs))\n",
    "    for start_idx in tqdm_utils.tqdm_notebook_failsafe(range(0, len(inputs) - batchsize + 1, batchsize)):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "train_log = []\n",
    "val_log = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24\n",
      "Train accuracy: 0.09864\n",
      "Val accuracy: 0.0991\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(25):\n",
    "\n",
    "    for x_batch,y_batch in iterate_minibatches(X_train,y_train,batchsize=32,shuffle=True):\n",
    "        train(network,x_batch,y_batch)\n",
    "    \n",
    "    train_log.append(np.mean(predict(network,X_train)==y_train))\n",
    "    val_log.append(np.mean(predict(network,X_val)==y_val))\n",
    "    \n",
    "    clear_output()\n",
    "    print(\"Epoch\",epoch)\n",
    "    print(\"Train accuracy:\",train_log[-1])\n",
    "    print(\"Val accuracy:\",val_log[-1])\n",
    "    plt.plot(train_log,label='train accuracy')\n",
    "    plt.plot(val_log,label='val accuracy')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    plt.grid()\n",
    "    \n",
    "    plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "264px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
